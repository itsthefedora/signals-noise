% Signals & Noise
% Deterministic LTI systems theory


\chapter{Linearity and translation-invariance}

A monophonic record contains information about sound encoded in a groove etched into the vinyl, with the sound's amplitude corresponding to the height of the groove. This physical geometry of the record has to be transformed into sound that reaches your ear: on today's systems, the groove is first transduced into an electric current through a needle, and then this electric current is transformed into sound through vibrating elements in speakers.

At each stage, the information content is transformed into a new form; however, at least to a first approximation, each transformation posesses two fundamentally important qualities. First, the transformations are \newterm{linear}: that is, a doubling of the height of the groove results in a doubling of the transduced electric current, and hence a doubling of the resultant sound amplitude out of the speakers; and, similarly, were we to add together the groove heights generated by two different sounds on our platter, the resultant output at the speaker would be the superposition of those two sounds. Second, the transformations are \newterm{translation-invariant} in time: if you were to plaly the record tomorrow, it would sound exactly the same as it would if you played it today. Our goal in this section is to characterize transformations of information that retain these two properties. As we will see, despite their apparent simplicity, linearity and translation-invariance actually provide immense constraints on the behavior of general transformations.

It's important to note that in real world apparatuses, these properties only hold approximately. In our example, linearity breaks down for a number of reasons: the record is only of finite thickness, and hence there is a breakdown of the transduction process when the groove height becomes too large; what's more, due to practical limitations on operational amplifier circuits, the process that drives the speakers from transduced electrical currents suffers from nonlinear distortion, especially at high frequencies, resulting in a characteristic ``crunchy'' sound. Translation invariance is clearly limited in our example as well: if we translated far enough \emph{back} in time, we would be attempting to play the record before the record player was made; and, if we tried to play our record millions of years in the future, our turntable would clearly be degraded to the point of no longer functioning. However, many practical apparatuses have an operational regime in which their behavior is very approximately linear and/or translation-invariant; and, as we will see, the beautiful mathematics imparted by these properties provides numerous practical benefits for analysis and understanding.

\section{Signals and systems}

Let's begin by defining our objects of study. First, we would like to formally characterize ``information content'', the object being acted upon by our transformations of interest; for example, this information may be electrical voltage fluctuations over time, or patterns of light passing through a 2D aperture, or the pressure of air induced by a speaker at every point in a room. Formally, we call such information content a \newterm{signal}, which we define as a ``nice'' function
\[ \func{u}{G}{\reals} \]
where we impose on $G$ the structure of a \emph{topological group}, meaning that $G$ posesses a topological structure (open sets) and a group structure (a binary operation $\blank + \blank$ and an inverse $- \blank$) that are ``compatible'' with one another, in the sense that both of the functions $\blank + \blank$ and $- \blank$ are continuous in $G$'s topology. We denote the set of all such signals on $G$ by $S(G)$; for the time being, we will leave it nebulous as to what functions are ``allowed'' signals, and which are too ``ugly'' to be dealt with, a point which we will return to once we have a few more tools at our disposal.

Although we won't use much of this formalism \emph{per se}, it can be helpful to think of this topological group structure as being the ``minimal'' amount of structure we need to impose on the domain of our signal in order for translation to ``make sense''. The group operation \emph{is}, in a manner of speaking, the translation: for example, when $G$ is $\reals$ (as in, for example, the case where we have signals that vary over \emph{time}), then the group operations on $\reals$ (\ie, $+$ and $-$) anchor the behavior of translation forward and backward \emph{in time} to the addition and subtraction \emph{of timestamps} in $\reals$; more specifically, for any $\tau \in \reals$, we can construct a transformation $\func{T_\tau}{\feval{S}{\reals}}{\feval{S}{\reals}}$ defined by
\[ \feval{T_\tau}{\feval{f}{t}} = \feval{f}{t - \tau},\,\forall t \]
This transformation is a \newterm{translation by $\tau$}, and is evidently fundamentally linked to the subtraction (\ie, addition and negation) on $\reals$.

Although it is possibble to develop a theory of signals for arbitrary topological groups $G$, as we've begun to explore, the most practical examples come from the cases $\reals^n$ (called \newterm{continuous signals}) and $\integers^n$ (called \newterm{discrete signals}), with addition (equipped with negation) as the group operation. Note, however, that no matter what $G$ is, we will always be interested in selecting our set of ``allowed'' signals $S(G)$ to be a vector space over $\reals$, using pointwise scalar multiplication and addition as the vector space operations; this structure will become indispensable.

As stated at the beginning, however, our direct interest is not necessarily with the signals themselves, but with the way in which one signal is transformed into another---for example, the transduction of mechanical motion of the turntable needle into an electrical signal. We call such a transformation of signals a \newterm{system},
\[ \func{A}{\feval{S}{F}}{\feval{S}{G}} \]
(noting that the case $F = G$ is of particular interest for our study). Systems in general have very little governing mathematical structure: there is a wide gamut of different systems having vastly different timbres. However, as noted in the introduction, there are a few classes of systems that are remarkably well-behaved.

First, we note that, as in the case of metric spaces, the most immediately interesting mappings between objects are those that preserve those objects' underlying mathematical structure. The most readily apparent structure on $\feval{S}{G}$, as we noted above, is its \emph{vector space} structure over $\reals$; hence, most immediately, it would seem that we would first want to consider those systems that are vector space homomorphisms (that is, linear operators) between signal spaces.
\begin{definition}[Linear system]
A system $\func{A}{\feval{S}{F}}{\feval{S}{G}}$ is \newterm{linear} if and only if
\begin{enumerate}
    \item $A$ preserves scalar multiplication; that is, for any $f \in \feval{S}{F}$, $\alpha \in \reals$,
    \[ \feval{A}{\alpha f} = \alpha \feval{A}{f} \]
    \item $A$ preserves vector addition; that is, for any $f,g \in \feval{S}{F}$,
    \[ \feval{A}{f + g} = \feval{A}{f} + \feval{A}{g} \]
\end{enumerate}
\end{definition}

Linear systems are a rich field of investigation, and are the general objects of study in linear algebra. \ldots

\ldots

$\feval{S}{G}$ has more structure than that of a vector space, however: we baked in the fact that $G$ is a topological group! Because of this, it also seems reasonable for us to consider systems that, in a sense, preserve this structure. First, we generalize our construction of a transformation that ``shifts'' a signal from earlier: given the signal space $\feval{S}{G}$, where $G$'s group operation is $\blank + \blank$ and inverse is $- \blank$, as well as an $x \in G$, the \newterm{translation by $x$} system $\func{T_x}{\feval{S}{G}}{\feval{S}{G}}$ is defined, for all $f \in \feval{S}{G}$, by
\[ \feval{T_x}{\feval{f}{g}} = \feval{f}{g + (-x)},\,\forall g \in G \]

Note that translation systems are actually quite well behaved:
\begin{claim}
For any $G$ and $x \in G$, the translation system $\func{T_x}{\feval{S}{G}}{\feval{S}{G}}$ is a linear operator.
\end{claim}
\begin{proof}
(Left as exercise.)
\end{proof}

Now that we have these systems induced by $G$'s group structure, we can easily find systems that ``respect'' the induced shifts:
\begin{definition}[Translation-invariant system]
A system $\func{A}{\feval{S}{G}}{\feval{S}{G}}$ is \newterm{translation-invariant} if and only if, for any $f \in \feval{S}{G}$ and $x \in G$:
\[ \feval{\left(A \compose T_x\right)}{f} = \feval{\left(T_x \compose A\right)}{f} \]
\end{definition}
That is, for a translation-invariant system, shifting the input gives an identical output, shifted by the same amount; put another way, translation-invariant systems are precisely those that commute with the translation operators.

\begin{example}
As an exercise, consider the following closely-related problems: 

\begin{enumerate}
    \item Consider the linear difference equation
    \[ \fevald{y}{n} = \alpha \fevald{y}{n - 1} + \fevald{u}{n} \]
    (The notation $\fevald{f}{n}$ is common to distinguish discrete signals from their continuous counterparts.) Consider also the additional constraint of zero initial conditions for $y$; \ie,
    \[ \lim_{n \to -\infty} \fevald{y}{n} = 0 \]
    Prove that the system $\func{A}{\feval{S}{\integers}}{\feval{S}{\integers}}$ induced by letting $\feval{A}{u}$ be the solution $y$ of this difference equation is both linear and time-invariant.
    \item Further, consider the forced linear differential equation
    \[ \feval{\dot{y}}{t} = \alpha \feval{y}{t} + \feval{u}{t} \]
    with the constraint of zero initial conditions for $y$; \ie,
    \[ \lim_{t \to -\infty} \feval{y}{t} = 0 \]
    Prove that the system $\func{A}{\feval{S}{\reals}}{\feval{S}{\reals}}$ induced by letting $\feval{A}{u}$ be the solution $y$ of this differential equation is both linear and time-invariant.
\end{enumerate}

\end{example}

\ldots

For brevity, we denote systems that posess both linearity and translation-invariance \newterm{LTI systems}; in some texts, these are denoted \newterm{linear shift-invariant (LSI) systems}.

\ldots

\section{Discrete systems in one dimension}

Let's focus our attention for now on an example case: one-dimensional discrete signals, $\func{S}{\integers}$, and LTI systems between them.

We begin by developing a representation of an arbitrary discrete signal $\fevald{u}{n}$ as a sum of translated copies of simpler functions; the hope is that, if we can define the behavior of an LTI system on the simpler functions, we can extend the result to the full function by leveraging linearity and translation-invariance. One such simple function is the \newterm{unit impulse} (also known as the \newterm{Dirac delta}):
\[
\fevald{\delta}{n} = 
\begin{cases}
    1, & n = 0 \\
    0, & \textrm{otherwise}
\end{cases}
\]
(The importance of this particular choice of simple function will build as we develop our theory!)

With the unit impulse in hand, note that we can ``build up'' any discrete signal $\fevald{u}{n}$ by placing a scaled and shifted impulse at each time point:
\[ \fevald{u}{n} = \sum_{k=-\infty}^{\infty} u_k \, \fevald{\delta}{n - k} \]
where the scaling coefficients $u_k = \fevald{u}{k}$ now encode $u$'s information content. Now, consider an LTI system $A$'s action on $u$: letting $\fevald{\delta_k}{n} = \fevald{\left(T_k \, \delta \right)}{n} = \fevald{\delta}{n - k}$ be the impulse shifted by $k$, since
\[ u = \sum_k u_k \, \delta_k \]
by linearity we must have
\[ A \, u = \sum_k u_k \, \left(A \, \delta_k \right) \]
Hence, $A$'s behavior is entirely determined by how it transforms translated impulses. But wait, there's more! By translation-invariance, we must also have
\[ \fevald{\left(A\,\delta_k\right)}{n} = \fevald{\left( \left(A \compose T_k\right) \, \delta \right)}{n} = \fevald{\left( \left(T_k \compose A\right) \, \delta \right)}{n} = \fevald{\left(A\,\delta\right)}{n-k} \]
That is, $A$'s action on translated impulses is completely determined by its action on the standard impulse.

Putting it all together, we have (in ``vector'' form):
\[ A \, u = \sum_k u_k \, T_k \, \left( A \, \delta \right) \]
Amazing! $A$'s behavior is fully specified by $A \, \delta$, its action on the unit impulse; by scaling and translating this response to the impulse, we can obtain $A$'s output for any arbitrary input signal $u$. This special output signal is denoted the \newterm{impulse response} of $A$:
\[ h_A = A \, \delta \]
Recalling that $u_k$ is really just $\fevald{u}{k}$ in disguise, we can write the equation above in ``coordinate'' form:
\begin{eqnarray*}
    \fevald{\left(A \, u\right)}{n} & = & \sum_k \fevald{u}{k} \, \fevald{\left( T_k \, h_A \right)}{n} \\
      & = & \sum_k \fevald{u}{k} \, \fevald{h_A}{n - k} 
\end{eqnarray*}
And so we get a glimpse at what $A$ is doing: $A$'s output for $u$ is a superposition of scaled and shifted copies of $h_A$, $A$'s impulse response, where each copy (indexed by $k$) is scaled by the corresponding value of $u$ for that shift.

The operation performed by $A$ between $u$ and the impulse resposnse $h_A$ is of its own importance:
\begin{definition}[Convolution (discrete)]
Given two discrete signals $x, y \in \feval{S}{\integers}$, the \newterm{convolution} signal, $x \convolve y \in \feval{S}{\integers}$, is defined by
\[ \fevald{\left(x \convolve y\right)}{n} = \sum_{k = -\infty}^{\infty} \fevald{x}{k} \, \fevald{y}{n - k} \]
\end{definition}
With this definition in hand, we see immediately that the action of an LTI system is to convolve input signals with the system's impulse response:
\[ A \, u = u \convolve h_A\]
How delightfully parsimonious!

Convoution brings some algebraic structure of its own, being a binary operation on signals. (As we will see in the continuous case, this structure can be highly nontrivial!) For example:
\begin{claim}
Convolution is commutative; that is, $\forall x,y \in \feval{S}{\integers}$,
\[ x \convolve y = y \convolve x \]
\end{claim}
\begin{proof}
(Left as exercise.)
\end{proof}
As you may have already pieced together, the unit impulse is the identity for discrete convolution:
\[ \fevald{\left(\delta \convolve x\right)}{n} = \fevald{\left(x \convolve \delta\right)}{n} = \sum_k \fevald{x}{k} \, \fevald{\delta}{n - k} = \fevald{x}{n} \]

\ldots

\subsection{The $z$ transform and the discrete Fourier transform}

We've now seen that the behavior of LTI systems is extremely well-constrained as compared to the general case: LTI systems perform one function, and that is convolution with an impulse response function. But, let's not forget that these are \emph{linear operators}; hence, we can characterize their behavior from the perspective of vector spaces as well, not just from the perspective of the convolution algebra we introduced above. So, then, the first question is this: what is the spectrum of an LTI system? What are its eigenvectors? In the general case of linear operators, this question can get quite gnarly; however, as we will see, LTI systems are exceptionally well-behaved in this regard.

Consider a discrete signal given by the exponential
\[ \fevald{u}{n} = z^n \]
for some $z \in \reals$. Then, for an LTI system $A$, we have
\[ A\,u = u \convolve h_A = h_A \convolve u \]
which, in coordinates, becomes
\begin{eqnarray*}
    \fevald{\left(A\,u\right)}{n} & = & \sum_k \fevald{h_A}{k} \, \fevald{u}{n-k} \\
      & = & \sum_k \fevald{h_A}{k} \, z^{n - k} \\
      & = & \left( \sum_k \fevald{h_A}{k} z^{-k} \right) \, z^n \\
      & = & \feval{H_A}{z} \, \fevald{u}{n}
\end{eqnarray*}
And so we see that $z^n$ is an eigenfunction of $A$, with $\feval{H_A}{z}$ as the corresponding eigenvalue! (One must, of course, consider carefully the convergence of the sum that reduces to $H_A$; we will cover this point in more detail as we progress.)

We can, however, go even further: why not consider values of $z$ that are in $\complex$ as well? We can still evaluate our system's convolution in the same way, and when we do, we still arrive at the same result: $z^n$ for complex $z$ is also an eigenfunction of $A$ in the same way. (Why?) With this extended definition, we have extracted another crucial object describing our LTI system $A$. The \emph{eigenvalue} of $A$ corresponding to the eigenfunction $z^n, z \in \complex$ in the expression above yields a map $\func{H_A}{\complex}{\complex}$ defined by
\[ \feval{H_A}{z} \defeq \sum_k \fevald{h_A}{k} z^{-k} \]
This function is known as the \newterm{$z$-transform} of $A$'s impulse response (and hence, the $z$-transform corresponding to $A$). We will see that, just like the impulse response, the $z$-transform encodes all of the information about $A$'s action on inputs; however, the form of that information is much more conducive to analysis.

\begin{example}
The translation by 1 system, also called the \newterm{unit delay},
\[ \fevald{\left(T_1\,x\right)}{n} = \fevald{x}{n-1} \]
is LTI, with impulse response
\[ \fevald{h_{T_1}}{n} = \fevald{\delta}{n-1} \]
The corresponding $z$-transform is
\[ \fevald{H_{T_1}}{z} = z^{-1} \]
(which is generally used in engineering texts as the symbol for a unit delay).
\end{example}
\begin{proof}
(Exercise.)
\end{proof}

\section{Higher dimensions}

\ldots

\section{Continuous systems}

\ldots

\section{General considerations and Pontryagin duality}

\ldots